Q206. On which of data sets should we use unsupervised learning techniques?

Q207. What is clustering? Give a real world sernario where you can apply clustering.

Q208. What is the difference between classification and clustering?

Q209. Expalin how K-Means clusters the data sets.

Q210. What is Voronoi tessellation?

Q211. How can we measure the distance from each instance to every centroid?

Q212. What is the drawback if we randomly initialize the centroids?

Q213. What are the methods for centroid initialization?

Q214. What is acclerated K-Means clustering?

Q215. What is the advantage of acclerated K-Means clustering over K-Means clustering?

Q216. What is mini batch K-Means clustering?

Q217. How mini batch K-Means clustering acclerates the model training?

Q218. What are the alternative if your data does not fit into the memory of your system?

Q219. How can you find the optimal number of clusters?

Q220. What is the use silhouette score?

Q221. What is the use silhouette coefficient?

Q222. List some limitation of K-Means clustering.

Q223. How can clustering be used for Image Segmentation?

Q224. Can we use clustering for preprocessing in ML lifecycle? If no why, if yes how?

Q225. Use clustering algorithm as a demo for sem-supervised learning.

Q226. What is active learning?

Q227. Expalin DBSCAN.

Q228. What is local density based estimation?

Q229. Can we predict which cluster a new data set belongs to using predict method in DBSCAN. Support your answer with appropriate reasons?

Q230. How is prediction done in DBSCAN?

Q231. Do outliers affect DBSCAN performance?

Q232. Explain HDBSCAN (Hierarchical DBSCAN).

Q233. Explain Agglomerative clustering.

Q234. What is bottom up approach?

Q235. Explain BIRCH ((Balanced Iterative Reducing and Clustering using Hierarchies) algorithm.

Q236. Explain Mean-Shift algorithm.

Q237. What is Affinity propagation?

Q238. What is Spectral clustering?

Q239. What is Gaussian Distribution?

Q240. What is Gaussian Mixture Model?

Q241. Expalin the working of Gaussian Mixture Model.

Q242. What is anomaly?

Q243. What are the techniques that can be used to find anomaly in the data set?

Q244. How can you use Gaussian Mixture for anomaly detection?

Q245. What is likelihood function?

Q246. Explain Bayesian Gaussian Mixture Models.

Q247. List the difference between Gaussian Mixture Model and Bayesian Gaussian Mixture Model.

Q248. How can you use Fast-MCD(minimum covariance determinant) for anomaly detection?

Q249. How can you use Isolation Forest for anomaly detection?

Q250. How can you use LOF(Local Outlier Factor) for anomaly detection?

Q251. How can you use One-class SVM for anomaly detection?

Q252. What is label propagation? Why would you implement it, and how?

Q253. Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?

Q254. Can you think of a use case where active learning would be useful? How would
you implement it?

Q255. What is the difference between anomaly detection and novelty detection?

Q256. The [Olivetti faces dataset](https://www.kaggle.com/code/serkanpeldek/face-recognition-on-olivetti-dataset) contains 400 grayscale 64 × 64–pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that
can predict which person is represented in each picture. Load the dataset, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using K-Means, and ensure that you have a good number of clusters.
Visualize the clusters: do you see similar faces in each cluster?

Q257. Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?

Q258. Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the
algorithm, you should probably reduce the dataset’s dimensionality (e.g., use
PCA, preserving 99% of the variance). Use the model to generate some new faces
(using the sample() method), and visualize them (if you used PCA, you will need
to use its inverse_transform() method). Try to modify some images (e.g.,
rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare
the output of the score_samples() method for normal images and for anomalies).

Q259. Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image.
Next, take some of the modified images you built in the previous exercise, and
look at their reconstruction error: notice how much larger the reconstruction
error is. If you plot a reconstructed image, you will see why: it tries to reconstruct
a normal face.
